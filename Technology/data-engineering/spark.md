# Spark

## Overview

Spark is an engine for large-scale data processing. It was first created to address the limitations of Hadoop MapReduce which used distributed computing to process extremely large datasets but relied on disk storage and batch processing. Spark instead uses in-memory storage and computation allowing it to be much faster than Hadoop for processing big data.

When harnessing multiple computers (cluster) for handling big data, Spark manages and coordinates tasks for each computer in the cluster. The data is divided into partitions to be sent to each machine

## Architecture

## PySpark

PySpark is the Spark API for Python
